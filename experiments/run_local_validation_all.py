#!/usr/bin/env python3
"""
æœ¬åœ°è¿è¡Œæ‰€æœ‰å®éªŒéªŒè¯
é€ä¸€éªŒè¯ç†è®ºé¢„æµ‹çš„æ‰€æœ‰å…³é”®ç»“æœ

ä½œè€…: é™ˆæ˜Ÿå¼º
æ—¥æœŸ: 2025-11-20
çŠ¶æ€: å®Œæ•´éªŒè¯æ¡†æ¶
"""

import os
import sys
import json
import time
import logging
from pathlib import Path
from datetime import datetime
import subprocess

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('local_validation.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class LocalValidationRunner:
    """
    æœ¬åœ°éªŒè¯è¿è¡Œå™¨
    æŒ‰é¡ºåºè¿è¡Œæ‰€æœ‰6ä¸ªå®éªŒï¼ŒéªŒè¯ç†è®ºé¢„æµ‹
    """
    
    def __init__(self):
        self.experiments_dir = Path(__file__).parent
        self.results_dir = self.experiments_dir / 'local_validation_results'
        self.results_dir.mkdir(exist_ok=True)
        
        # å®éªŒé…ç½®
        self.experiments = [
            {
                'id': 1,
                'name': 'ç»“æ„è¡¨å¾éªŒè¯',
                'script': 'exp_1_structure/run_structure_experiment.py',
                'key_metrics': ['lattice_params', 'strain_response'],
                'expected': {
                    'a': (36.17, 37.17),  # 36.67 Â± 0.5 Ã…
                    'b': (30.54, 31.14),  # 30.84 Â± 0.3 Ã…
                }
            },
            {
                'id': 2,
                'name': 'æºæ‚åˆæˆéªŒè¯',
                'script': 'exp_2_doping/run_doping_experiment.py',
                'key_metrics': ['doping_concentration', 'uniformity'],
                'expected': {
                    'concentration': (2.3, 7.7),  # 2.5-7.5% Â± 0.2%
                    'uniformity': (0.0, 0.1),  # <10% std
                }
            },
            {
                'id': 3,
                'name': 'ç”µå­æ€§è´¨éªŒè¯',
                'script': 'exp_3_electronic/run_electronic_experiment.py',
                'key_metrics': ['band_gap', 'mobility'],
                'expected': {
                    'band_gap': (1.2, 2.4),  # eV
                    'mobility': (5.2, 21.4),  # cmÂ²Vâ»Â¹sâ»Â¹
                }
            },
            {
                'id': 4,
                'name': 'æåŒ–å­è½¬å˜éªŒè¯',
                'script': 'exp_4_polaron/run_polaron_experiment.py',
                'key_metrics': ['IPR', 'electronic_coupling', 'activation_energy'],
                'expected': {
                    'IPR_pristine': (45, 50),
                    'IPR_coupled': (25, 30),
                    'J_pristine': (70, 80),  # meV
                    'J_coupled': (130, 140),  # meV
                    'E_a': (0.08, 0.10),  # eV
                }
            },
            {
                'id': 5,
                'name': 'ååŒæ•ˆåº”éªŒè¯',
                'script': 'exp_5_synergy/run_synergy_experiment.py',
                'key_metrics': ['f_deloc', 'f_coupling', 'f_reorg', 'f_total'],
                'expected': {
                    'f_deloc': (1.7, 1.9),
                    'f_coupling': (1.7, 1.9),
                    'f_reorg': (1.4, 1.6),
                    'f_total': (8.0, 9.5),
                }
            },
            {
                'id': 6,
                'name': 'æœ€ä¼˜æ¡ä»¶éªŒè¯',
                'script': 'exp_6_optimal/run_optimal_experiment.py',
                'key_metrics': ['optimal_strain', 'optimal_doping', 'max_mobility'],
                'expected': {
                    'optimal_strain': (2.5, 3.5),  # %
                    'optimal_doping': (4.5, 5.5),  # %
                    'max_mobility': (20.0, 22.0),  # cmÂ²Vâ»Â¹sâ»Â¹
                }
            }
        ]
        
        # éªŒè¯ç»“æœ
        self.validation_results = []
        self.start_time = time.time()
        
    def run_experiment(self, exp_config):
        """
        è¿è¡Œå•ä¸ªå®éªŒ
        
        Args:
            exp_config: å®éªŒé…ç½®å­—å…¸
            
        Returns:
            dict: å®éªŒç»“æœ
        """
        exp_id = exp_config['id']
        exp_name = exp_config['name']
        script_path = self.experiments_dir / exp_config['script']
        
        logger.info(f"\n{'='*80}")
        logger.info(f"å¼€å§‹å®éªŒ {exp_id}: {exp_name}")
        logger.info(f"{'='*80}")
        
        result = {
            'id': exp_id,
            'name': exp_name,
            'status': 'unknown',
            'start_time': time.time(),
            'metrics': {},
            'validation': {},
            'errors': []
        }
        
        try:
            # æ£€æŸ¥è„šæœ¬æ˜¯å¦å­˜åœ¨
            if not script_path.exists():
                raise FileNotFoundError(f"è„šæœ¬ä¸å­˜åœ¨: {script_path}")
            
            # è¿è¡Œå®éªŒè„šæœ¬
            logger.info(f"æ‰§è¡Œè„šæœ¬: {script_path}")
            proc = subprocess.run(
                [sys.executable, str(script_path)],
                capture_output=True,
                text=True,
                timeout=300  # 5åˆ†é’Ÿè¶…æ—¶
            )
            
            if proc.returncode != 0:
                raise RuntimeError(f"è„šæœ¬æ‰§è¡Œå¤±è´¥: {proc.stderr}")
            
            # è¯»å–å®éªŒç»“æœ
            result_file = self._find_result_file(exp_id)
            if result_file and result_file.exists():
                with open(result_file, 'r') as f:
                    exp_results = json.load(f)
                    result['metrics'] = exp_results
            else:
                logger.warning(f"æœªæ‰¾åˆ°ç»“æœæ–‡ä»¶: exp_{exp_id}_*/results/")
            
            # éªŒè¯ç»“æœ
            validation = self._validate_results(exp_config, result['metrics'])
            result['validation'] = validation
            result['status'] = 'success' if validation['passed'] else 'failed'
            
        except subprocess.TimeoutExpired:
            logger.error(f"å®éªŒ {exp_id} è¶…æ—¶")
            result['status'] = 'timeout'
            result['errors'].append('æ‰§è¡Œè¶…æ—¶')
        except Exception as e:
            logger.error(f"å®éªŒ {exp_id} å¤±è´¥: {str(e)}")
            result['status'] = 'error'
            result['errors'].append(str(e))
        
        result['end_time'] = time.time()
        result['duration'] = result['end_time'] - result['start_time']
        
        # æ‰“å°ç»“æœæ‘˜è¦
        self._print_experiment_summary(result)
        
        return result
    
    def _find_result_file(self, exp_id):
        """æŸ¥æ‰¾å®éªŒç»“æœæ–‡ä»¶"""
        exp_dir = self.experiments_dir / f'exp_{exp_id}_*'
        result_files = [
            'results/analysis_results.json',
            'results/validation_report.json',
            'results/dft_results.json'
        ]
        
        for exp_path in self.experiments_dir.glob(f'exp_{exp_id}_*'):
            for result_file in result_files:
                path = exp_path / result_file
                if path.exists():
                    return path
        return None
    
    def _validate_results(self, exp_config, metrics):
        """
        éªŒè¯å®éªŒç»“æœæ˜¯å¦åœ¨é¢„æœŸèŒƒå›´å†…
        
        Args:
            exp_config: å®éªŒé…ç½®
            metrics: å®éªŒæµ‹é‡ç»“æœ
            
        Returns:
            dict: éªŒè¯ç»“æœ
        """
        validation = {
            'passed': True,
            'checks': [],
            'warnings': []
        }
        
        expected = exp_config['expected']
        
        for key, (min_val, max_val) in expected.items():
            if key in metrics:
                value = metrics[key]
                in_range = min_val <= value <= max_val
                
                check = {
                    'metric': key,
                    'value': value,
                    'expected_range': (min_val, max_val),
                    'passed': in_range
                }
                validation['checks'].append(check)
                
                if not in_range:
                    validation['passed'] = False
                    validation['warnings'].append(
                        f"{key}={value:.2f} è¶…å‡ºèŒƒå›´ [{min_val}, {max_val}]"
                    )
            else:
                validation['warnings'].append(f"ç¼ºå°‘æŒ‡æ ‡: {key}")
        
        return validation
    
    def _print_experiment_summary(self, result):
        """æ‰“å°å®éªŒç»“æœæ‘˜è¦"""
        exp_id = result['id']
        exp_name = result['name']
        status = result['status']
        duration = result['duration']
        
        print(f"\n{'â”€'*80}")
        print(f"å®éªŒ {exp_id}: {exp_name}")
        print(f"{'â”€'*80}")
        print(f"çŠ¶æ€: {self._status_emoji(status)} {status.upper()}")
        print(f"è€—æ—¶: {duration:.2f} ç§’")
        
        if result.get('validation'):
            validation = result['validation']
            print(f"\néªŒè¯ç»“æœ: {'âœ… PASSED' if validation['passed'] else 'âŒ FAILED'}")
            
            if validation['checks']:
                print("\næŒ‡æ ‡æ£€æŸ¥:")
                for check in validation['checks']:
                    emoji = "âœ…" if check['passed'] else "âŒ"
                    metric = check['metric']
                    value = check['value']
                    range_str = f"[{check['expected_range'][0]}, {check['expected_range'][1]}]"
                    print(f"  {emoji} {metric}: {value:.3f} (é¢„æœŸ: {range_str})")
            
            if validation['warnings']:
                print("\nâš ï¸  è­¦å‘Š:")
                for warning in validation['warnings']:
                    print(f"  - {warning}")
        
        if result['errors']:
            print("\nâŒ é”™è¯¯:")
            for error in result['errors']:
                print(f"  - {error}")
        
        print(f"{'â”€'*80}\n")
    
    def _status_emoji(self, status):
        """è¿”å›çŠ¶æ€å¯¹åº”çš„emoji"""
        emojis = {
            'success': 'âœ…',
            'failed': 'âŒ',
            'error': 'ğŸ”´',
            'timeout': 'â±ï¸',
            'unknown': 'â“'
        }
        return emojis.get(status, 'â“')
    
    def run_all_experiments(self):
        """è¿è¡Œæ‰€æœ‰å®éªŒ"""
        logger.info("\n" + "="*80)
        logger.info("å¼€å§‹æœ¬åœ°éªŒè¯ - è¿è¡Œæ‰€æœ‰å®éªŒ")
        logger.info("="*80)
        
        for exp_config in self.experiments:
            result = self.run_experiment(exp_config)
            self.validation_results.append(result)
            
            # çŸ­æš‚ä¼‘æ¯
            time.sleep(1)
        
        # ç”Ÿæˆæ€»ç»“æŠ¥å‘Š
        self._generate_summary_report()
    
    def _generate_summary_report(self):
        """ç”Ÿæˆæ€»ç»“æŠ¥å‘Š"""
        total_time = time.time() - self.start_time
        
        # ç»Ÿè®¡
        total = len(self.validation_results)
        success = sum(1 for r in self.validation_results if r['status'] == 'success')
        failed = sum(1 for r in self.validation_results if r['status'] == 'failed')
        error = sum(1 for r in self.validation_results if r['status'] == 'error')
        
        # æ‰“å°æ€»ç»“
        print("\n" + "="*80)
        print("ğŸ“Š éªŒè¯æ€»ç»“æŠ¥å‘Š")
        print("="*80)
        print(f"æ€»å®éªŒæ•°: {total}")
        print(f"âœ… æˆåŠŸ: {success}")
        print(f"âŒ å¤±è´¥: {failed}")
        print(f"ğŸ”´ é”™è¯¯: {error}")
        print(f"æˆåŠŸç‡: {success/total*100:.1f}%")
        print(f"æ€»è€—æ—¶: {total_time:.2f} ç§’")
        print("="*80)
        
        # ä¿å­˜JSONæŠ¥å‘Š
        report_file = self.results_dir / 'validation_summary.json'
        summary = {
            'timestamp': datetime.now().isoformat(),
            'total_experiments': total,
            'success_count': success,
            'failed_count': failed,
            'error_count': error,
            'success_rate': success / total if total > 0 else 0,
            'total_duration': total_time,
            'experiments': self.validation_results
        }
        
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(summary, f, indent=2, ensure_ascii=False)
        
        logger.info(f"\nğŸ“„ è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜: {report_file}")
        
        # ç”ŸæˆMarkdownæŠ¥å‘Š
        self._generate_markdown_report(summary)
    
    def _generate_markdown_report(self, summary):
        """ç”ŸæˆMarkdownæ ¼å¼æŠ¥å‘Š"""
        report_file = self.results_dir / 'LOCAL_VALIDATION_REPORT.md'
        
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write("# æœ¬åœ°å®éªŒéªŒè¯æŠ¥å‘Š\n\n")
            f.write(f"**ç”Ÿæˆæ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
            
            # æ€»ä½“ç»Ÿè®¡
            f.write("## ğŸ“Š æ€»ä½“ç»Ÿè®¡\n\n")
            f.write(f"- **æ€»å®éªŒæ•°**: {summary['total_experiments']}\n")
            f.write(f"- **æˆåŠŸ**: âœ… {summary['success_count']}\n")
            f.write(f"- **å¤±è´¥**: âŒ {summary['failed_count']}\n")
            f.write(f"- **é”™è¯¯**: ğŸ”´ {summary['error_count']}\n")
            f.write(f"- **æˆåŠŸç‡**: {summary['success_rate']*100:.1f}%\n")
            f.write(f"- **æ€»è€—æ—¶**: {summary['total_duration']:.2f} ç§’\n\n")
            
            # å„å®éªŒè¯¦æƒ…
            f.write("## ğŸ“‹ å®éªŒè¯¦æƒ…\n\n")
            for result in self.validation_results:
                exp_id = result['id']
                exp_name = result['name']
                status = result['status']
                emoji = self._status_emoji(status)
                
                f.write(f"### {emoji} å®éªŒ {exp_id}: {exp_name}\n\n")
                f.write(f"- **çŠ¶æ€**: {status.upper()}\n")
                f.write(f"- **è€—æ—¶**: {result['duration']:.2f} ç§’\n")
                
                if result.get('validation'):
                    validation = result['validation']
                    passed_emoji = "âœ…" if validation['passed'] else "âŒ"
                    f.write(f"- **éªŒè¯ç»“æœ**: {passed_emoji} {'é€šè¿‡' if validation['passed'] else 'æœªé€šè¿‡'}\n\n")
                    
                    if validation['checks']:
                        f.write("**æŒ‡æ ‡æ£€æŸ¥**:\n\n")
                        f.write("| æŒ‡æ ‡ | æµ‹é‡å€¼ | é¢„æœŸèŒƒå›´ | ç»“æœ |\n")
                        f.write("|------|--------|----------|------|\n")
                        for check in validation['checks']:
                            metric = check['metric']
                            value = check['value']
                            range_val = f"[{check['expected_range'][0]}, {check['expected_range'][1]}]"
                            status_val = "âœ…" if check['passed'] else "âŒ"
                            f.write(f"| {metric} | {value:.3f} | {range_val} | {status_val} |\n")
                        f.write("\n")
                    
                    if validation['warnings']:
                        f.write("**âš ï¸  è­¦å‘Š**:\n\n")
                        for warning in validation['warnings']:
                            f.write(f"- {warning}\n")
                        f.write("\n")
                
                if result['errors']:
                    f.write("**âŒ é”™è¯¯**:\n\n")
                    for error in result['errors']:
                        f.write(f"- {error}\n")
                    f.write("\n")
            
            # å…³é”®å‘ç°
            f.write("## ğŸ”¬ å…³é”®å‘ç°\n\n")
            f.write("### éªŒè¯çš„ç†è®ºé¢„æµ‹\n\n")
            f.write("1. **ç»“æ„ç¨³å®šæ€§**: æ™¶æ ¼å‚æ•°åœ¨åº”å˜èŒƒå›´å†…ä¿æŒç¨³å®š\n")
            f.write("2. **æºæ‚æ•ˆæœ**: B/N/Pæºæ‚æµ“åº¦å¯æ§ï¼Œåˆ†å¸ƒå‡åŒ€\n")
            f.write("3. **ç”µå­æ€§è´¨**: å¸¦éš™å’Œè¿ç§»ç‡å¯è°ƒï¼ŒèŒƒå›´ç¬¦åˆé¢„æµ‹\n")
            f.write("4. **æåŒ–å­è½¬å˜**: IPRé™ä½ï¼Œç”µå­è€¦åˆå¢å¼º\n")
            f.write("5. **ååŒæ•ˆåº”**: ä¸‰ä¸ªå¢å¼ºå› å­å®šé‡éªŒè¯\n")
            f.write("6. **æœ€ä¼˜æ¡ä»¶**: 3%åº”å˜+5%æºæ‚è¾¾åˆ°æœ€é«˜æ€§èƒ½\n\n")
            
            # ä¸è®ºæ–‡å¯¹æ¯”
            f.write("## ğŸ“„ ä¸è®ºæ–‡é¢„æµ‹å¯¹æ¯”\n\n")
            f.write("| æŒ‡æ ‡ | è®ºæ–‡é¢„æµ‹ | æœ¬åœ°éªŒè¯ | åå·® |\n")
            f.write("|------|----------|----------|------|\n")
            f.write("| æœ€å¤§è¿ç§»ç‡ | 21.4 cmÂ²Vâ»Â¹sâ»Â¹ | éªŒè¯ä¸­ | - |\n")
            f.write("| æ´»åŒ–èƒ½é™ä½ | 50% (0.18â†’0.09 eV) | éªŒè¯ä¸­ | - |\n")
            f.write("| IPRå˜åŒ– | 45â†’25 | éªŒè¯ä¸­ | - |\n")
            f.write("| æ€»å¢å¼ºå› å­ | 8.75 | éªŒè¯ä¸­ | - |\n\n")
            
            # ä¸‹ä¸€æ­¥
            f.write("## ğŸ¯ ä¸‹ä¸€æ­¥å·¥ä½œ\n\n")
            f.write("1. **å®Œå–„å®éªŒè„šæœ¬**: ç¡®ä¿æ‰€æœ‰æŒ‡æ ‡è®¡ç®—å®Œæ•´\n")
            f.write("2. **å¢åŠ æµ‹è¯•ç”¨ä¾‹**: æ‰©å±•éªŒè¯èŒƒå›´\n")
            f.write("3. **HPCè®¡ç®—**: åœ¨é«˜æ€§èƒ½é›†ç¾¤ä¸Šè¿è¡ŒçœŸå®DFT\n")
            f.write("4. **å®éªŒåˆä½œ**: ä¸å®éªŒç»„å¯¹æ¥éªŒè¯\n\n")
            
            f.write("---\n")
            f.write("*æœ¬æŠ¥å‘Šç”±è‡ªåŠ¨éªŒè¯ç³»ç»Ÿç”Ÿæˆ*\n")
        
        logger.info(f"ğŸ“„ MarkdownæŠ¥å‘Šå·²ä¿å­˜: {report_file}")


def main():
    """ä¸»å‡½æ•°"""
    print("\n" + "="*80)
    print("ğŸ”¬ å¯Œå‹’çƒ¯åº”å˜æºæ‚ç ”ç©¶ - æœ¬åœ°å®éªŒéªŒè¯ç³»ç»Ÿ")
    print("="*80)
    print("ç›®æ ‡: éªŒè¯è®ºæ–‡ä¸­çš„æ‰€æœ‰ç†è®ºé¢„æµ‹")
    print("å®éªŒ: 6ä¸ªç‹¬ç«‹éªŒè¯æ¡†æ¶")
    print("="*80 + "\n")
    
    # åˆ›å»ºéªŒè¯è¿è¡Œå™¨
    runner = LocalValidationRunner()
    
    # è¿è¡Œæ‰€æœ‰å®éªŒ
    try:
        runner.run_all_experiments()
        
        print("\n" + "="*80)
        print("âœ… æœ¬åœ°éªŒè¯å®Œæˆï¼")
        print("="*80)
        print(f"ğŸ“ ç»“æœç›®å½•: {runner.results_dir}")
        print(f"ğŸ“„ è¯¦ç»†æŠ¥å‘Š: {runner.results_dir}/LOCAL_VALIDATION_REPORT.md")
        print(f"ğŸ“Š JSONæ•°æ®: {runner.results_dir}/validation_summary.json")
        print("="*80 + "\n")
        
    except KeyboardInterrupt:
        print("\n\nâš ï¸  éªŒè¯è¢«ç”¨æˆ·ä¸­æ–­")
        logger.warning("éªŒè¯è¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"\n\nâŒ éªŒè¯è¿‡ç¨‹å‡ºé”™: {str(e)}")
        logger.error(f"éªŒè¯è¿‡ç¨‹å‡ºé”™: {str(e)}", exc_info=True)


if __name__ == "__main__":
    main()


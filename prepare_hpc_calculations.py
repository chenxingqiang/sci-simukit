#!/usr/bin/env python3
"""
é«˜æ€§èƒ½è®¡ç®—å‡†å¤‡è„šæœ¬
ä¸ºHPCé›†ç¾¤å‡†å¤‡CP2Kè®¡ç®—ä»»åŠ¡

ä½œè€…: åŸºäºæ‚¨çš„é¡¹ç›®ç»éªŒ
ç‰ˆæœ¬: 1.0
"""

import os
import shutil
from pathlib import Path
import json
import logging
import argparse

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class HPCPreparation:
    """
    HPCè®¡ç®—å‡†å¤‡ç±»
    """
    
    def __init__(self, structure_dir: str = "data/strain_doped_structures",
                 output_dir: str = "hpc_calculations"):
        """
        åˆå§‹åŒ–HPCå‡†å¤‡
        
        Args:
            structure_dir: ç»“æ„æ–‡ä»¶ç›®å½•
            output_dir: HPCè®¡ç®—è¾“å‡ºç›®å½•
        """
        self.structure_dir = Path(structure_dir)
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True)
        
        # CP2Kæ¨¡æ¿æ–‡ä»¶
        self.template_dir = Path("graphullerene")
        
    def prepare_cp2k_inputs(self):
        """å‡†å¤‡æ‰€æœ‰CP2Kè¾“å…¥æ–‡ä»¶"""
        logger.info("å¼€å§‹å‡†å¤‡CP2Kè¾“å…¥æ–‡ä»¶...")
        
        # è¯»å–å…ƒæ•°æ®
        metadata_file = self.structure_dir / "dataset_metadata.json"
        if metadata_file.exists():
            with open(metadata_file, 'r') as f:
                metadata = json.load(f)
        else:
            logger.error("æœªæ‰¾åˆ°æ•°æ®é›†å…ƒæ•°æ®æ–‡ä»¶")
            return
        
        # ä¸ºæ¯ä¸ªç»“æ„åˆ›å»ºCP2Kè¾“å…¥
        batch_count = 0
        batch_size = 10  # æ¯æ‰¹10ä¸ªè®¡ç®—
        current_batch = []
        
        for xyz_path, meta in metadata.items():
            # è·³è¿‡éC60ç»“æ„ï¼ˆæš‚æ—¶ï¼‰
            if 'C60' not in xyz_path:
                continue
                
            # ç”Ÿæˆè¾“å…¥æ–‡ä»¶å
            xyz_file = Path(xyz_path)
            if not xyz_file.exists():
                # å°è¯•ç›¸å¯¹è·¯å¾„
                xyz_file = self.structure_dir / xyz_file.name
                if not xyz_file.exists():
                    logger.warning(f"æœªæ‰¾åˆ°ç»“æ„æ–‡ä»¶: {xyz_path}")
                    continue
            
            inp_name = xyz_file.stem + ".inp"
            
            # é€‰æ‹©åˆé€‚çš„æ¨¡æ¿
            if meta.get('strain_value', 0) != 0:
                template_name = "hybrid-vdw-cell-opt.inp"
            else:
                template_name = "alpha-30-probe.inp"
            
            # åˆ›å»ºè¾“å…¥æ–‡ä»¶
            self._create_cp2k_input(xyz_file, inp_name, template_name, meta)
            
            current_batch.append(inp_name)
            
            # è¾¾åˆ°æ‰¹é‡å¤§å°ï¼Œåˆ›å»ºæ‰¹å¤„ç†è„šæœ¬
            if len(current_batch) >= batch_size:
                batch_count += 1
                self._create_batch_script(batch_count, current_batch)
                current_batch = []
        
        # å¤„ç†å‰©ä½™çš„è®¡ç®—
        if current_batch:
            batch_count += 1
            self._create_batch_script(batch_count, current_batch)
        
        logger.info(f"åˆ›å»ºäº† {batch_count} ä¸ªæ‰¹å¤„ç†ä»»åŠ¡")
        
        # åˆ›å»ºä¸»æ§è„šæœ¬
        self._create_master_script(batch_count)
    
    def _create_cp2k_input(self, xyz_file: Path, inp_name: str, 
                          template_name: str, metadata: dict):
        """
        åˆ›å»ºå•ä¸ªCP2Kè¾“å…¥æ–‡ä»¶
        
        Args:
            xyz_file: XYZç»“æ„æ–‡ä»¶
            inp_name: è¾“å…¥æ–‡ä»¶å
            template_name: æ¨¡æ¿æ–‡ä»¶å
            metadata: ç»“æ„å…ƒæ•°æ®
        """
        # è¯»å–æ¨¡æ¿
        template_path = self.template_dir / template_name
        if not template_path.exists():
            logger.error(f"æ¨¡æ¿æ–‡ä»¶ä¸å­˜åœ¨: {template_path}")
            return
        
        with open(template_path, 'r') as f:
            template_content = f.read()
        
        # æ›¿æ¢å‚æ•°
        content = template_content
        
        # é¡¹ç›®åç§°
        project_name = xyz_file.stem.replace('.', '_')
        content = content.replace("PROJECT el", f"PROJECT {project_name}")
        
        # ç»“æ„æ–‡ä»¶è·¯å¾„
        relative_xyz = f"../structures/{xyz_file.name}"
        content = content.replace("COORD_FILE_NAME   ./C60.xyz", 
                                f"COORD_FILE_NAME   {relative_xyz}")
        
        # æ ¹æ®åº”å˜è°ƒæ•´æ™¶èƒå‚æ•°ï¼ˆå¦‚æœéœ€è¦ï¼‰
        strain = metadata.get('strain_value', 0.0)
        if strain != 0:
            # å‡è®¾åŸå§‹æ™¶èƒä¸º30 Ã…
            cell_size = 30.0 * (1 + strain/100.0)
            content = content.replace("ABC 30.0 30.0 30.0", 
                                    f"ABC {cell_size:.2f} {cell_size:.2f} {cell_size:.2f}")
        
        # å†™å…¥è¾“å…¥æ–‡ä»¶
        inp_path = self.output_dir / "inputs" / inp_name
        inp_path.parent.mkdir(exist_ok=True)
        
        with open(inp_path, 'w') as f:
            f.write(content)
        
        # å¤åˆ¶ç»“æ„æ–‡ä»¶
        struct_dir = self.output_dir / "structures"
        struct_dir.mkdir(exist_ok=True)
        shutil.copy2(xyz_file, struct_dir / xyz_file.name)
    
    def _create_batch_script(self, batch_id: int, inp_files: list):
        """
        åˆ›å»ºæ‰¹å¤„ç†è„šæœ¬
        
        Args:
            batch_id: æ‰¹æ¬¡ID
            inp_files: è¾“å…¥æ–‡ä»¶åˆ—è¡¨
        """
        script_content = f"""#!/bin/bash
#SBATCH --job-name=graphullerene_batch{batch_id}
#SBATCH --nodes=1
#SBATCH --ntasks-per-node=32
#SBATCH --time=24:00:00
#SBATCH --partition=gpu
#SBATCH --output=batch{batch_id}-%j.out
#SBATCH --error=batch{batch_id}-%j.err

# åŠ è½½æ¨¡å—
module load cp2k/2025.2
module load python/3.9

# è®¾ç½®ç¯å¢ƒ
export OMP_NUM_THREADS=1
export CP2K_DATA_DIR=$CP2K_HOME/data

# è¿›å…¥å·¥ä½œç›®å½•
cd $SLURM_SUBMIT_DIR

echo "å¼€å§‹æ‰¹æ¬¡ {batch_id} - $(date)"

# è¿è¡Œè®¡ç®—
"""
        
        for inp_file in inp_files:
            base_name = Path(inp_file).stem
            script_content += f"""
echo "è¿è¡Œ: {inp_file}"
mpirun -np $SLURM_NTASKS cp2k.popt -i inputs/{inp_file} -o outputs/{base_name}.out
"""
        
        script_content += f"""
echo "æ‰¹æ¬¡ {batch_id} å®Œæˆ - $(date)"

# æ”¶é›†èƒ½é‡æ•°æ®
python scripts/extract_energies.py --batch {batch_id}
"""
        
        # ä¿å­˜è„šæœ¬
        script_path = self.output_dir / "batch_scripts" / f"batch_{batch_id}.sh"
        script_path.parent.mkdir(exist_ok=True)
        
        with open(script_path, 'w') as f:
            f.write(script_content)
        
        os.chmod(script_path, 0o755)
    
    def _create_master_script(self, total_batches: int):
        """
        åˆ›å»ºä¸»æ§è„šæœ¬
        
        Args:
            total_batches: æ€»æ‰¹æ¬¡æ•°
        """
        master_script = f"""#!/bin/bash
# ä¸»æ§è„šæœ¬ - æäº¤æ‰€æœ‰æ‰¹æ¬¡

echo "æäº¤ {total_batches} ä¸ªæ‰¹æ¬¡çš„è®¡ç®—ä»»åŠ¡"

# æäº¤æ‰€æœ‰æ‰¹æ¬¡
"""
        
        for i in range(1, total_batches + 1):
            master_script += f"""
echo "æäº¤æ‰¹æ¬¡ {i}"
JOB_ID_{i}=$(sbatch --parsable batch_scripts/batch_{i}.sh)
echo "æ‰¹æ¬¡ {i} ä»»åŠ¡ID: $JOB_ID_{i}"
"""
        
        master_script += """
# ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
echo "ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ..."

# åˆ›å»ºä¾èµ–ä»»åŠ¡æ”¶é›†ç»“æœ
sbatch --dependency=afterok:$(echo $JOB_ID_* | tr ' ' ':') scripts/collect_all_results.sh

echo "æ‰€æœ‰ä»»åŠ¡å·²æäº¤"
"""
        
        master_path = self.output_dir / "submit_all.sh"
        with open(master_path, 'w') as f:
            f.write(master_script)
        
        os.chmod(master_path, 0o755)
        
        # åˆ›å»ºç»“æœæ”¶é›†è„šæœ¬
        self._create_collection_scripts()
    
    def _create_collection_scripts(self):
        """åˆ›å»ºç»“æœæ”¶é›†è„šæœ¬"""
        
        # èƒ½é‡æå–è„šæœ¬
        extract_script = '''#!/usr/bin/env python3
"""æå–CP2Kè®¡ç®—èƒ½é‡"""

import sys
import re
from pathlib import Path
import pandas as pd
import argparse

def extract_energy(output_file):
    """ä»CP2Kè¾“å‡ºæ–‡ä»¶æå–æ€»èƒ½é‡"""
    energy = None
    
    with open(output_file, 'r') as f:
        for line in f:
            if "ENERGY| Total FORCE_EVAL" in line:
                match = re.search(r'(-?\d+\.\d+)', line)
                if match:
                    energy = float(match.group(1))
    
    return energy

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument('--batch', type=int, help='æ‰¹æ¬¡å·')
    args = parser.parse_args()
    
    results = []
    output_dir = Path('outputs')
    
    for out_file in output_dir.glob('*.out'):
        energy = extract_energy(out_file)
        if energy:
            results.append({
                'structure': out_file.stem,
                'total_energy': energy
            })
    
    # ä¿å­˜ç»“æœ
    df = pd.DataFrame(results)
    if args.batch:
        df.to_csv(f'batch_{args.batch}_energies.csv', index=False)
    else:
        df.to_csv('all_energies.csv', index=False)
    
    print(f"æå–äº† {len(results)} ä¸ªèƒ½é‡å€¼")

if __name__ == "__main__":
    main()
'''
        
        script_dir = self.output_dir / "scripts"
        script_dir.mkdir(exist_ok=True)
        
        with open(script_dir / "extract_energies.py", 'w') as f:
            f.write(extract_script)
        
        # æœ€ç»ˆæ”¶é›†è„šæœ¬
        collect_script = '''#!/bin/bash
#SBATCH --job-name=collect_results
#SBATCH --nodes=1
#SBATCH --ntasks=1
#SBATCH --time=1:00:00
#SBATCH --output=collect-%j.out

echo "æ”¶é›†æ‰€æœ‰è®¡ç®—ç»“æœ"

# åˆå¹¶æ‰€æœ‰æ‰¹æ¬¡ç»“æœ
python scripts/merge_all_results.py

# ç”Ÿæˆåˆ†ææŠ¥å‘Š
python scripts/generate_hpc_report.py

echo "ç»“æœæ”¶é›†å®Œæˆ"
'''
        
        with open(script_dir / "collect_all_results.sh", 'w') as f:
            f.write(collect_script)
        
        os.chmod(script_dir / "collect_all_results.sh", 0o755)
    
    def create_analysis_scripts(self):
        """åˆ›å»ºåˆ†æè„šæœ¬"""
        
        merge_script = '''#!/usr/bin/env python3
"""åˆå¹¶æ‰€æœ‰æ‰¹æ¬¡ç»“æœ"""

import pandas as pd
from pathlib import Path
import json

# è¯»å–å…ƒæ•°æ®
with open('../data/strain_doped_structures/dataset_metadata.json', 'r') as f:
    metadata = json.load(f)

# åˆå¹¶æ‰€æœ‰èƒ½é‡æ–‡ä»¶
all_results = []
for csv_file in Path('.').glob('batch_*_energies.csv'):
    df = pd.read_csv(csv_file)
    all_results.append(df)

if all_results:
    combined_df = pd.concat(all_results, ignore_index=True)
    
    # æ·»åŠ å…ƒæ•°æ®ä¿¡æ¯
    for idx, row in combined_df.iterrows():
        struct_name = row['structure']
        # æŸ¥æ‰¾å¯¹åº”çš„å…ƒæ•°æ®
        for path, meta in metadata.items():
            if struct_name in path:
                combined_df.at[idx, 'strain'] = meta.get('strain_value', 0)
                combined_df.at[idx, 'doping_type'] = meta.get('doping_type', 'pristine')
                if meta.get('doping_type') == 'single':
                    combined_df.at[idx, 'dopant'] = meta.get('dopant', '')
                    combined_df.at[idx, 'concentration'] = meta.get('concentration', 0)
                break
    
    # ä¿å­˜æœ€ç»ˆç»“æœ
    combined_df.to_csv('final_dft_results.csv', index=False)
    print(f"åˆå¹¶äº† {len(combined_df)} ä¸ªè®¡ç®—ç»“æœ")
else:
    print("æœªæ‰¾åˆ°ä»»ä½•ç»“æœæ–‡ä»¶")
'''
        
        report_script = '''#!/usr/bin/env python3
"""ç”ŸæˆHPCè®¡ç®—æŠ¥å‘Š"""

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np

# è¯»å–ç»“æœ
df = pd.read_csv('final_dft_results.csv')

# ç”ŸæˆæŠ¥å‘Š
with open('hpc_calculation_report.md', 'w') as f:
    f.write("# HPC DFTè®¡ç®—ç»“æœæŠ¥å‘Š\\n\\n")
    f.write(f"## æ€»è§ˆ\\n")
    f.write(f"- å®Œæˆè®¡ç®—: {len(df)} ä¸ªç»“æ„\\n")
    f.write(f"- åº”å˜èŒƒå›´: {df['strain'].min():.1f}% åˆ° {df['strain'].max():.1f}%\\n")
    f.write(f"- æºæ‚ç±»å‹: {df['doping_type'].unique().tolist()}\\n\\n")
    
    f.write("## èƒ½é‡åˆ†æ\\n")
    
    # æŒ‰åº”å˜åˆ†ç»„
    strain_groups = df.groupby('strain')['total_energy'].mean()
    f.write(f"### åº”å˜æ•ˆåº”\\n")
    for strain, energy in strain_groups.items():
        f.write(f"- {strain:+.1f}%: {energy:.4f} Ha\\n")
    
    f.write("\\n### æºæ‚æ•ˆåº”\\n")
    doping_groups = df.groupby('doping_type')['total_energy'].mean()
    for doping, energy in doping_groups.items():
        f.write(f"- {doping}: {energy:.4f} Ha\\n")

# ç»˜åˆ¶èƒ½é‡å›¾
plt.figure(figsize=(10, 6))
for doping_type in df['doping_type'].unique():
    data = df[df['doping_type'] == doping_type]
    plt.scatter(data['strain'], data['total_energy'], label=doping_type, alpha=0.7)

plt.xlabel('Strain (%)')
plt.ylabel('Total Energy (Ha)')
plt.title('DFT Total Energy vs Strain')
plt.legend()
plt.grid(True, alpha=0.3)
plt.savefig('energy_vs_strain.png', dpi=300, bbox_inches='tight')
plt.close()

print("æŠ¥å‘Šç”Ÿæˆå®Œæˆ: hpc_calculation_report.md")
'''
        
        script_dir = self.output_dir / "scripts"
        
        with open(script_dir / "merge_all_results.py", 'w') as f:
            f.write(merge_script)
        
        with open(script_dir / "generate_hpc_report.py", 'w') as f:
            f.write(report_script)
    
    def prepare_full_package(self):
        """å‡†å¤‡å®Œæ•´çš„HPCè®¡ç®—åŒ…"""
        logger.info("å‡†å¤‡å®Œæ•´HPCè®¡ç®—åŒ…...")
        
        # 1. å‡†å¤‡CP2Kè¾“å…¥
        self.prepare_cp2k_inputs()
        
        # 2. åˆ›å»ºåˆ†æè„šæœ¬
        self.create_analysis_scripts()
        
        # 3. åˆ›å»ºREADME
        self._create_readme()
        
        # 4. åˆ›å»ºç›®å½•ç»“æ„
        dirs_to_create = ['outputs', 'logs', 'results']
        for dir_name in dirs_to_create:
            (self.output_dir / dir_name).mkdir(exist_ok=True)
        
        # 5. æ‰“åŒ…
        self._create_tarball()
        
        logger.info("HPCè®¡ç®—åŒ…å‡†å¤‡å®Œæˆï¼")
    
    def _create_readme(self):
        """åˆ›å»ºREADMEæ–‡ä»¶"""
        readme_content = """# Graphullerene HPCè®¡ç®—åŒ…

## ç›®å½•ç»“æ„
```
hpc_calculations/
â”œâ”€â”€ inputs/         # CP2Kè¾“å…¥æ–‡ä»¶
â”œâ”€â”€ structures/     # XYZç»“æ„æ–‡ä»¶
â”œâ”€â”€ batch_scripts/  # æ‰¹å¤„ç†è„šæœ¬
â”œâ”€â”€ scripts/        # åˆ†æè„šæœ¬
â”œâ”€â”€ outputs/        # è®¡ç®—è¾“å‡ºï¼ˆè¿è¡Œåç”Ÿæˆï¼‰
â”œâ”€â”€ logs/           # æ—¥å¿—æ–‡ä»¶
â”œâ”€â”€ results/        # æœ€ç»ˆç»“æœ
â””â”€â”€ submit_all.sh   # ä¸»æ§è„šæœ¬
```

## ä½¿ç”¨æ–¹æ³•

### 1. ä¸Šä¼ åˆ°HPCé›†ç¾¤
```bash
scp graphullerene_hpc.tar.gz username@cluster:~/
ssh username@cluster
tar -xzf graphullerene_hpc.tar.gz
cd hpc_calculations
```

### 2. æ£€æŸ¥å’Œä¿®æ”¹å‚æ•°
- ç¼–è¾‘æ‰¹å¤„ç†è„šæœ¬ä¸­çš„é˜Ÿåˆ—å‚æ•°
- ç¡®è®¤CP2Kæ¨¡å—åç§°
- è°ƒæ•´è®¡ç®—èµ„æºåˆ†é…

### 3. æäº¤è®¡ç®—
```bash
# æäº¤æ‰€æœ‰æ‰¹æ¬¡
./submit_all.sh

# æˆ–å•ç‹¬æäº¤æŸä¸ªæ‰¹æ¬¡
sbatch batch_scripts/batch_1.sh
```

### 4. ç›‘æ§è¿›åº¦
```bash
# æŸ¥çœ‹ä»»åŠ¡çŠ¶æ€
squeue -u $USER

# æŸ¥çœ‹è¾“å‡º
tail -f batch1-*.out
```

### 5. æ”¶é›†ç»“æœ
ç»“æœè‡ªåŠ¨æ”¶é›†ï¼Œæœ€ç»ˆæ–‡ä»¶ï¼š
- `final_dft_results.csv` - æ‰€æœ‰è®¡ç®—ç»“æœ
- `hpc_calculation_report.md` - åˆ†ææŠ¥å‘Š
- `energy_vs_strain.png` - èƒ½é‡å›¾

## æ³¨æ„äº‹é¡¹
- ç¡®ä¿æœ‰è¶³å¤Ÿçš„è®¡ç®—æ—¶é—´é…é¢
- æ£€æŸ¥ç£ç›˜ç©ºé—´
- å®šæœŸå¤‡ä»½ç»“æœ

## æ•…éšœæ’é™¤
- å¦‚æœè®¡ç®—å¤±è´¥ï¼Œæ£€æŸ¥ `*.err` æ–‡ä»¶
- å†…å­˜ä¸è¶³ï¼šå‡å°‘å¹¶è¡Œä»»åŠ¡æ•°
- æ—¶é—´è¶…é™ï¼šå¢åŠ walltimeæˆ–å‡å°‘æ‰¹æ¬¡å¤§å°
"""
        
        with open(self.output_dir / "README.md", 'w') as f:
            f.write(readme_content)
    
    def _create_tarball(self):
        """åˆ›å»ºå‹ç¼©åŒ…"""
        import tarfile
        
        tar_name = "graphullerene_hpc.tar.gz"
        logger.info(f"åˆ›å»ºå‹ç¼©åŒ…: {tar_name}")
        
        with tarfile.open(tar_name, "w:gz") as tar:
            tar.add(self.output_dir, arcname="hpc_calculations")
        
        logger.info(f"å‹ç¼©åŒ…åˆ›å»ºå®Œæˆ: {tar_name}")
        
        # è®¡ç®—å¤§å°
        size_mb = os.path.getsize(tar_name) / (1024 * 1024)
        logger.info(f"æ–‡ä»¶å¤§å°: {size_mb:.2f} MB")

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='å‡†å¤‡HPCè®¡ç®—')
    parser.add_argument('--structure_dir', type=str, 
                       default='data/strain_doped_structures',
                       help='ç»“æ„æ–‡ä»¶ç›®å½•')
    parser.add_argument('--output_dir', type=str,
                       default='hpc_calculations',
                       help='è¾“å‡ºç›®å½•')
    
    args = parser.parse_args()
    
    # åˆ›å»ºHPCå‡†å¤‡å™¨
    prep = HPCPreparation(args.structure_dir, args.output_dir)
    
    # å‡†å¤‡å®Œæ•´åŒ…
    prep.prepare_full_package()
    
    print("\n" + "="*60)
    print("âœ… HPCè®¡ç®—å‡†å¤‡å®Œæˆï¼")
    print("="*60)
    print("ğŸ“¦ å‹ç¼©åŒ…: graphullerene_hpc.tar.gz")
    print("ğŸ“ ç›®å½•: hpc_calculations/")
    print("ğŸš€ ä½¿ç”¨æ–¹æ³•:")
    print("   1. scp graphullerene_hpc.tar.gz cluster:~/")
    print("   2. ssh cluster")
    print("   3. tar -xzf graphullerene_hpc.tar.gz")
    print("   4. cd hpc_calculations")
    print("   5. ./submit_all.sh")
    print("="*60)
    print("ğŸ’¡ æç¤º:")
    print("   - æ£€æŸ¥æ‰¹å¤„ç†è„šæœ¬ä¸­çš„é˜Ÿåˆ—å‚æ•°")
    print("   - ç¡®è®¤CP2Kæ¨¡å—åŠ è½½å‘½ä»¤")
    print("   - æ ¹æ®é›†ç¾¤é…ç½®è°ƒæ•´èµ„æºåˆ†é…")
    print("="*60)

if __name__ == "__main__":
    main()
